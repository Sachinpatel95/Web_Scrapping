{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mini_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTvfYCaWmUuv",
        "outputId": "80acc03a-27ef-4728-c4b3-874dbe172e60"
      },
      "source": [
        "import requests\n",
        "import os\n",
        "from bs4 import BeautifulSoup, SoupStrainer\n",
        "# Makes Output Directory if it does not exist\n",
        "if not os.path.exists(os.path.join(os.getcwd(), 'HackerNews')):\n",
        "    os.makedirs(os.path.join(os.getcwd(), 'HackerNews'))\n",
        "\n",
        "def fetch(page_no, verbose=False):\n",
        "    # Should be unreachable, but just in case\n",
        "    if page_no <= 0:\n",
        "        raise ValueError('Number of Pages must be greater than zero')\n",
        "    page_no = min(page_no, 20)\n",
        "    i = page_no\n",
        "    if verbose:\n",
        "        print('Fetching Page {}...'.format(i))\n",
        "    try:\n",
        "        res = requests.get('https://news.ycombinator.com/?p=' + str(i))\n",
        "        only_td = SoupStrainer('td')\n",
        "        soup = BeautifulSoup(res.content, 'html.parser', parse_only=only_td)\n",
        "        tdtitle = soup.find_all('td', attrs={'class': 'title'})\n",
        "        tdmetrics = soup.find_all('td', attrs={'class': 'subtext'})\n",
        "       \n",
        "        with open(os.path.join('HackerNews', 'NewsPage{}.txt'.format(i)), 'w+') as f:\n",
        "            f.write('-' * 80)\n",
        "            f.write('\\n')\n",
        "            f.write('Page {}'.format(i))\n",
        "            tdtitle = soup.find_all('td', attrs={'class': 'title'})\n",
        "            tdrank = soup.find_all(\n",
        "                'td',\n",
        "                attrs={\n",
        "                    'class': 'title',\n",
        "                    'align': 'right'})\n",
        "            tdtitleonly = [t for t in tdtitle if t not in tdrank]\n",
        "            tdmetrics = soup.find_all('td', attrs={'class': 'subtext'})\n",
        "            tdt = tdtitleonly\n",
        "            tdr = tdrank\n",
        "            tdm = tdmetrics\n",
        "            num_iter = min(len(tdr), len(tdt))\n",
        "            for idx in range(num_iter):\n",
        "                f.write('\\n' + '-' * 80 + '\\n')\n",
        "                rank = tdr[idx].find('span', attrs={'class': 'rank'})\n",
        "                titl = tdt[idx].find('a', attrs={'class': 'storylink'})\n",
        "                #print(titl)\n",
        "                # url = titl['href'] if titl and titl['href'].startswith('https') else 'https://news.ycombinator.com/' + titl['href']\n",
        "                \n",
        "                site = tdt[idx].find('span', attrs={'class': 'sitestr'})\n",
        "                score = tdm[idx].find('span', attrs={'class': 'score'})\n",
        "                time = tdm[idx].find('span', attrs={'class': 'age'})\n",
        "                author = tdm[idx].find('a', attrs={'class': 'hnuser'})\n",
        "                f.write(\n",
        "                    '\\nArticle Number: ' +\n",
        "                    rank.text.replace(\n",
        "                        '.',\n",
        "                        '') if rank else '\\nArticle Number: Could not get article number')\n",
        "                f.write(\n",
        "                    '\\nArticle Title: ' +\n",
        "                    titl.text if titl else '\\nArticle Title: Could not get article title')\n",
        "                f.write(\n",
        "                    '\\nSource Website: ' +\n",
        "                    site.text if site else '\\nSource Website: https://news.ycombinator.com')\n",
        "                # f.write(\n",
        "                #     '\\nSource URL: ' +\n",
        "                #     url if url else '\\nSource URL: No URL found for this article')\n",
        "                f.write(\n",
        "                    '\\nArticle Author: ' +\n",
        "                    author.text if author else '\\nArticle Author: Could not get article author')\n",
        "                f.write(\n",
        "                    '\\nArticle Score: ' +\n",
        "                    score.text if score else '\\nArticle Score: Not Scored')\n",
        "                f.write(\n",
        "                    '\\nPosted: ' +\n",
        "                    time.text if time else '\\nPosted: Could not find when the article was posted')\n",
        "                f.write('\\n' + '-' * 80 + '\\n')\n",
        "    except (requests.ConnectionError, requests.packages.urllib3.exceptions.ConnectionError) as e:\n",
        "        print('Connection Failed for page {}'.format(i))\n",
        "    except requests.RequestException as e:\n",
        "        print(\"Some ambiguous Request Exception occurred. The exception is \" + str(e))\n",
        "\n",
        "\n",
        "while(True):\n",
        "    try:\n",
        "        pages = int(\n",
        "            input('Enter number of pages that you want the HackerNews for (max 20): '))\n",
        "        v = input('Want verbose output y/[n] ?')\n",
        "        verbose = v.lower().startswith('y')\n",
        "        if pages > 20:\n",
        "            print('A maximum of only 20 pages can be fetched')\n",
        "        pages = min(pages, 20)\n",
        "        for page_no in range(1, pages + 1):\n",
        "            fetch(page_no, verbose)\n",
        "        break\n",
        "    except ValueError:\n",
        "        print('\\nInvalid input, probably not a positive integer\\n')\n",
        "        continue"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter number of pages that you want the HackerNews for (max 20): 6\n",
            "Want verbose output y/[n] ?y\n",
            "Fetching Page 1...\n",
            "Fetching Page 2...\n",
            "Fetching Page 3...\n",
            "Fetching Page 4...\n",
            "Fetching Page 5...\n",
            "Fetching Page 6...\n"
          ]
        }
      ]
    }
  ]
}